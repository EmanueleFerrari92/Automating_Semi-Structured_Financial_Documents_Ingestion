{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pymupdf4llm\n",
    "import pandas as pd\n",
    "import fitz\n",
    "import os\n",
    "import numpy as np\n",
    "import fitz\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_markdown(text):\n",
    "    # try decode from latin1\n",
    "    try:\n",
    "        text = text.encode('latin1').decode('latin1')\n",
    "    except:\n",
    "        pass\n",
    "    # try decode from ascii\n",
    "    try:\n",
    "        text = text.encode('ascii').decode('ascii')\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "\n",
    "    # decode escape characters\n",
    "    # text = text.encode().decode('unicode_escape')\n",
    "\n",
    "    # use regex to add a new line at the end of a line starting with #, if the following line do not start with #\n",
    "    text = re.sub(r'(\\s#.*)(?=\\n[^\\s#])', r'\\1\\n', text) \n",
    "\n",
    "    # if match ** + only nonprintable characters excluding new line + ** replace with ' '\n",
    "    text = re.sub(r'\\*\\*[\\W&&[^\\n]]+\\*\\*', ' ', text) \n",
    "    # same for __\n",
    "    text = re.sub(r'_[\\W&&[^\\n]]+_', ' ', text).strip()   \n",
    "\n",
    "    # separate line with all bold\n",
    "    pattern = r'(^|\\s*\\n)(\\*\\*[^\\n]*?\\*\\*)( ?\\n)((\\s*[^\\*])|$)'\n",
    "    replacement = r'\\1\\n\\2\\n\\3\\4'\n",
    "    text = re.sub(pattern, replacement, text, flags=re.MULTILINE)\n",
    "    # separete line with all italic\n",
    "    pattern = r'(^|\\s*\\n)(_[^\\n]*?_)( ?\\n)((\\s*[^_])|$)'\n",
    "    replacement = r'\\1\\n\\2\\n\\3\\4'\n",
    "    text = re.sub(pattern, replacement, text, flags=re.MULTILINE)\n",
    "\n",
    "    # remove multiple spaces\n",
    "    text = re.sub(r' {2,}', ' ', text)\n",
    "    # collapse more than 2 new lines into 2\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def create_document_df(doc, filename):\n",
    "    rows = []\n",
    "\n",
    "    # Process Document and Populate DataFrame\n",
    "    doc_id = (filename.split('/')[-1]).split('.')[0].strip()\n",
    "    id = 0\n",
    "\n",
    "    for i, page in enumerate(doc):\n",
    "        text = page['text']\n",
    "        text = clean_markdown(text)\n",
    "        pos_in_page = 0\n",
    "\n",
    "        for text_block in text.split('\\n\\n'):\n",
    "            text_block = text_block.strip()\n",
    "            # if is empty skip it\n",
    "            if text_block == '':\n",
    "                continue\n",
    "            # if it has no alphanumeric characters, skip it\n",
    "            if not any(c.isalnum() for c in text_block):\n",
    "                continue\n",
    "\n",
    "            # clean text = markdown replace #, *, _, - characters with space, collapse multiple spaces\n",
    "            clean_text = re.sub(r'[\\#\\*\\_\\-]', ' ', text_block)\n",
    "            clean_text = re.sub(r' {2,}', ' ', clean_text)\n",
    "            # collapse multiple whitespaces\n",
    "            clean_text = re.sub(r'\\s+', ' ', clean_text)\n",
    "            clean_text = clean_text.strip().lower()\n",
    "\n",
    "            id += 10\n",
    "\n",
    "            # Append to list of rows\n",
    "            rows.append({\n",
    "                'doc_id': doc_id,\n",
    "                'page_number': i,\n",
    "                'pos_in_page': pos_in_page,\n",
    "                'md_text': text_block,\n",
    "                'clean_text': clean_text,\n",
    "                'id': id,\n",
    "                'item': None,\n",
    "                'is_noise': False,\n",
    "                'noise_type': None,\n",
    "                'type': None,\n",
    "                'emb': None,            \n",
    "            })  \n",
    "            \n",
    "            pos_in_page += 1\n",
    "    \n",
    "    # Create DataFrame from list of rows\n",
    "    df = pd.DataFrame(rows, columns=['doc_id', 'page_number', 'pos_in_page', 'md_text', 'clean_text', 'item', 'type' ,'id', 'emb'])\n",
    "\n",
    "    df['is_noise'] = False\n",
    "    df['noise_type'] = None\n",
    "\n",
    "    return df\n",
    "\n",
    "def create_df(filename):\n",
    "  d = fitz.open(filename)\n",
    "  page = d[0]\n",
    "  width_points = page.rect.width\n",
    "  height_points = page.rect.height\n",
    "\n",
    "  doc = pymupdf4llm.to_markdown(filename,\n",
    "                                # pages=(range(25)),\n",
    "                                hdr_info=False,\n",
    "                                margins=[0, 0, 0, 50],  # remove margins\n",
    "                                page_width=width_points,         # let it detect real width\n",
    "                                # page_height=height_points,   \n",
    "                                # force_text=True,\n",
    "                              #   table_strategy='lines',\n",
    "                                page_chunks=True,  \n",
    "                                show_progress = False,\n",
    "                                # extract_words=True,\n",
    "                              )\n",
    "\n",
    "\n",
    "  df = create_document_df(doc, filename)\n",
    "\n",
    "  df = df.set_index('id', drop=True)\n",
    "\n",
    "  return df\n",
    "\n",
    "def clean_df(df):\n",
    "    doc_id = df['doc_id'].iloc[0]\n",
    "\n",
    "    # print('\\nCleaning document...', doc_id)\n",
    "\n",
    "    # Remove rows with empty clean_text\n",
    "    df = df[df['clean_text'] != ''].copy()\n",
    "\n",
    "    # REMOVE HEADERS\n",
    "    def remove_md_notation(text):\n",
    "        text = re.sub(r'[\\#\\*\\_]', ' ', text)\n",
    "        text = re.sub(r' {2,}', ' ', text)\n",
    "        return text.strip()\n",
    "    \n",
    "    df['md'] = df['md_text'].apply(remove_md_notation)\n",
    "\n",
    "\n",
    "    def remove_4digit_years(text: str) -> str:\n",
    "        \"\"\"\n",
    "        Removes (or masks) any 4-digit year from the text.\n",
    "        \"\"\"\n",
    "        return re.sub(r'\\b\\d{4}\\b', '', text)\n",
    "\n",
    "\n",
    "    def label_repeated_headers(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Labels repeated headers in a DataFrame based on the following conditions:\n",
    "        1) Page number >= 3\n",
    "        2) Position in page <= 3\n",
    "        3) Does NOT contain the word \"item\" (case-insensitive)\n",
    "        4) Text repeats on the *immediately next* page\n",
    "            (page_number difference == 1)\n",
    "        5) The first occurrence of each distinct header is not labeled;\n",
    "            only subsequent consecutive pages are labeled.\n",
    "\n",
    "        The function sets:\n",
    "        - df['is_noise'] = True\n",
    "        - df['noise_type'] = 'header'\n",
    "        for repeated headers (but not for the first occurrence).\n",
    "        \"\"\"\n",
    "        # --- 1) Create candidate header mask ---\n",
    "        df['candidate_header'] = (\n",
    "            (df['page_number'] >= 3) &\n",
    "            (df['pos_in_page'] <= 3) &\n",
    "            (~df['md'].str.lower().str.contains('item'))\n",
    "        )\n",
    "\n",
    "        # --- 2) Create normalized text (remove 4-digit years, strip whitespace) ---\n",
    "        df['normalized_text'] = df['md'].apply(remove_4digit_years).str.strip()\n",
    "\n",
    "        # --- 3) Within each distinct normalized_text, track previous rowâ€™s page_number and candidate_header ---\n",
    "        #     - We only shift among rows that share the same normalized_text\n",
    "        #     - `previous_page` is the page_number of the last occurrence of this text\n",
    "        #     - `previous_candidate` tells us if that previous occurrence was also a candidate_header\n",
    "        df['previous_page'] = df.groupby('normalized_text')['page_number'].shift()\n",
    "        df['previous_candidate'] = df.groupby('normalized_text')['candidate_header'].shift()\n",
    "\n",
    "        # --- 4) Check if the current row is on the next page *and* both rows are candidate headers ---\n",
    "        #     - current row (df['candidate_header'] == True)\n",
    "        #     - previous row (df['previous_candidate'] == True)\n",
    "        #     - page_number difference = 1\n",
    "        df['header_repeats'] = (\n",
    "            df['candidate_header'] & \n",
    "\n",
    "            df['previous_candidate'].infer_objects(False) & \n",
    "            df['previous_page'].notna() &\n",
    "            ((df['page_number'] - df['previous_page']) == 1)\n",
    "        )\n",
    "\n",
    "        # --- 5) Label repeated headers ---\n",
    "        df['is_noise'] = df['header_repeats']\n",
    "        df['noise_type'] = df['header_repeats'].apply(lambda x: 'header' if x else None)\n",
    "\n",
    "        return df\n",
    "\n",
    "    df = label_repeated_headers(df)\n",
    "\n",
    "    df.drop(columns=['candidate_header', 'normalized_text', 'previous_page', 'header_repeats'], inplace=True)\n",
    "\n",
    "\n",
    "    # if there is at least one true in is_noise column\n",
    "    # if df['is_noise'].any():\n",
    "    #     print('Removing headers:')\n",
    "    # # print page number and md_text of noise type == 'header'\n",
    "    # for i, row in df[df['noise_type'] == 'header'].iterrows():\n",
    "    #     print('page:\\t', row['page_number'], '\\t', row['md_text'].replace('\\n', ' ').replace('  ', ' ').strip())\n",
    "\n",
    "\n",
    "\n",
    "    # ----------------------------\n",
    "    # 1) Helper: parse roman numerals\n",
    "    # ----------------------------\n",
    "    roman_map = {\n",
    "        'M': 1000, 'CM': 900, 'D': 500, 'CD': 400,\n",
    "        'C': 100, 'XC': 90, 'L': 50, 'XL': 40,\n",
    "        'X': 10, 'IX': 9, 'V': 5, 'IV': 4, 'I': 1\n",
    "    }\n",
    "\n",
    "    def roman_to_int(roman: str) -> int:\n",
    "        \"\"\"Converts a valid Roman numeral to integer. Returns -1 if invalid.\"\"\"\n",
    "        roman = roman.upper()\n",
    "        i, result = 0, 0\n",
    "        while i < len(roman):\n",
    "            if i+1<len(roman) and roman[i:i+2] in roman_map:\n",
    "                result += roman_map[roman[i:i+2]]\n",
    "                i += 2\n",
    "            elif roman[i] in roman_map:\n",
    "                result += roman_map[roman[i]]\n",
    "                i += 1\n",
    "            else:\n",
    "                return -1\n",
    "        return result\n",
    "\n",
    "    # ----------------------------\n",
    "    # 2) Helper: parse text block into (marker_type, marker_text, marker_value)\n",
    "    # ----------------------------\n",
    "    def parse_marker(text: str):\n",
    "        \"\"\"\n",
    "        Returns a tuple (marker_type, marker_text, marker_value) or (None, None, None)\n",
    "        if not matching any pattern.\n",
    "\n",
    "        marker_type can be: 'alpha', 'roman', 'numeric', 'mixed'.\n",
    "        marker_text is the static part for 'mixed' (otherwise '').\n",
    "        marker_value is an integer value representing the letter, roman, or digits.\n",
    "        \"\"\"\n",
    "        txt = text.strip()\n",
    "        \n",
    "        # a) Single letter (A, B, C, etc.)?\n",
    "        if re.fullmatch(r'[A-Za-z]', txt):\n",
    "            # Convert letter -> integer (A=1, B=2, etc.)\n",
    "            letter = txt.upper()\n",
    "            val = ord(letter) - ord('A') + 1\n",
    "            return ('alpha', '', val)\n",
    "\n",
    "        # b) Roman numeral?\n",
    "        #    let's see if it is a valid Roman\n",
    "        r_val = roman_to_int(txt)\n",
    "        if r_val > 0:\n",
    "            return ('roman', '', r_val)\n",
    "\n",
    "        # c) Pure numeric (1-3 digits)?\n",
    "        if re.fullmatch(r'\\d{1,3}', txt):\n",
    "            val = int(txt)\n",
    "            return ('numeric', '', val)\n",
    "\n",
    "        # d) Mixed: has exactly one 1-3 digit number, everything else is static\n",
    "        #    e.g. \"Chapter 1\", \"Chapter 2\", ...\n",
    "        #    We'll remove that 1-3 digit piece and see if the rest is consistent\n",
    "        #    We only handle 1 numeric group for simplicity\n",
    "        matches = re.findall(r'\\d{1,3}', txt)\n",
    "        if len(matches) == 1:\n",
    "            # Remove that numeric portion from the text to get the static part\n",
    "            # We'll replace all occurrences of that specific match just once or globally? \n",
    "            # If there's exactly 1 match, we can just do a re.sub for that match.\n",
    "            number_str = matches[0]\n",
    "            val = int(number_str)\n",
    "            # Carefully remove only the first occurrence of that digit substring\n",
    "            # or (commonly) all occurrences if youâ€™re sure there's only one:\n",
    "            static_part = re.sub(number_str, '', txt, count=1).strip()\n",
    "            # If there's some leftover text, we consider it the static part\n",
    "            # If it's empty, it basically means it's just a numeric -> covered above\n",
    "            if static_part:\n",
    "                return ('mixed', static_part, val)\n",
    "\n",
    "        # Otherwise, no match\n",
    "        return (None, None, None)\n",
    "\n",
    "    # ----------------------------\n",
    "    # 3) Label last 2 blocks per page that form sequences\n",
    "    # ----------------------------\n",
    "    def label_page_markers(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        For each page, consider only the last 2 blocks. \n",
    "        Parse them, check if there's a consecutive sequence across pages.\n",
    "        Label them as 'is_marker' = True if they form part of a recognized pattern\n",
    "        that is monotonically increasing on consecutive pages.\n",
    "        \n",
    "        We also label the subsequent block once we detect a sequence, \n",
    "        so the entire run of consecutive markers gets marked.\n",
    "        \"\"\"\n",
    "        # We'll do it in two stages:\n",
    "        #   Stage A: Identify the last 2 blocks of each page\n",
    "        #   Stage B: Parse each block, then look for consecutive patterns across pages\n",
    "\n",
    "        # --- Stage A: find last 2 blocks per page ---\n",
    "        # group by page_number, then pick the last 2 entries\n",
    "        # (assuming df is sorted by [page_number, pos_in_page])\n",
    "        df['rank_in_page_desc'] = df.groupby('page_number')['pos_in_page'] \\\n",
    "                                    .rank(method='first', ascending=False)\n",
    "        # rank_in_page_desc == 1 or 2 means the last 2 rows in that page\n",
    "        mask_last2 = df['rank_in_page_desc'].isin([1,2])\n",
    "        df['candidate_page_marker'] = mask_last2\n",
    "\n",
    "        # Prepare columns for storing marker info\n",
    "        df['marker_type'] = None\n",
    "        df['marker_text'] = None\n",
    "        df['marker_value'] = None\n",
    "        df['is_marker'] = False  # final label (True/False)\n",
    "\n",
    "        # --- Stage B: parse markers only if candidate_page_marker is True ---\n",
    "        # Then check consecutive pages\n",
    "        for idx, row in df.loc[mask_last2].iterrows():\n",
    "            (mtype, mtext, mval) = parse_marker(row['md_text'])\n",
    "            df.at[idx, 'marker_type'] = mtype\n",
    "            df.at[idx, 'marker_text'] = mtext\n",
    "            df.at[idx, 'marker_value'] = mval\n",
    "\n",
    "        # We'll now label consecutive sequences. We can do:\n",
    "        #   group by (marker_type, marker_text)\n",
    "        #   then examine consecutive rows in ascending page_number\n",
    "        #   check if page_number difference = 1\n",
    "        #   check if marker_value difference = 1\n",
    "        #   set is_marker = True for them (and their pair)\n",
    "\n",
    "        # For convenience, let's only keep the candidate rows with non-null marker_type\n",
    "        sub_df = df.loc[df['candidate_page_marker'] & df['marker_type'].notna()].copy()\n",
    "        sub_df.sort_values(by=['marker_type', 'marker_text', 'page_number'], inplace=True)\n",
    "\n",
    "        # We'll do a groupby\n",
    "        def detect_sequences(group):\n",
    "            \"\"\"\n",
    "            For each group (same marker_type, marker_text),\n",
    "            look for rows where marker_value is consecutive and page_number is consecutive.\n",
    "            Mark is_marker = True for all that appear in consecutive sequences.\n",
    "            \"\"\"\n",
    "            group = group.sort_values('page_number').copy()\n",
    "            group['prev_page']  = group['page_number'].shift()\n",
    "            group['prev_value'] = group['marker_value'].shift()\n",
    "\n",
    "            # Weâ€™ll track a boolean that is True if:\n",
    "            #   (page_number == prev_page + 1) and (marker_value == prev_value + 1)\n",
    "            # But we also want to label *both* sides of that link, i.e. the row and the previous row.\n",
    "            # We'll do it with a simple approach: if row is consecutive to the previous, mark both.\n",
    "\n",
    "            group['consecutive'] = False\n",
    "            for i in range(1, len(group)):\n",
    "                curr = group.iloc[i]\n",
    "                prev = group.iloc[i-1]\n",
    "                if (curr['page_number'] == prev['page_number'] + 1) and \\\n",
    "                (curr['marker_value'] == prev['marker_value'] + 1):\n",
    "                    # Mark both as part of the sequence\n",
    "                    group.at[group.index[i],   'consecutive'] = True\n",
    "                    group.at[group.index[i-1], 'consecutive'] = True\n",
    "\n",
    "            return group\n",
    "\n",
    "        labeled = sub_df.groupby(['marker_type', 'marker_text'], group_keys=False).apply(detect_sequences, include_groups=False)\n",
    "\n",
    "        # Merge the 'consecutive' info back into main df\n",
    "        df = df.merge(\n",
    "            labeled[['consecutive']],\n",
    "            how='left',\n",
    "            left_index=True,\n",
    "            right_index=True\n",
    "        )\n",
    "\n",
    "        # 'consecutive' = True means it's part of a recognized sequence. Let's set is_marker = True\n",
    "        df['is_marker'] = df['consecutive'].infer_objects(False)\n",
    "\n",
    "\n",
    "        # update is_noise\n",
    "        df['is_noise'] = df['is_noise'] | df['is_marker']\n",
    "        # noise type == 'page_number' when is marker is true\n",
    "        df['noise_type'] = np.where(df['is_marker'] == True, 'page_number', df['noise_type'])\n",
    "\n",
    "        # Clean up columns we don't need\n",
    "        df.drop(columns=['rank_in_page_desc', 'consecutive',], inplace=True, errors='ignore')\n",
    "\n",
    "        # marker value where the marker is not true (false or none) is set to none\n",
    "        df['marker_value'] = np.where(df['is_marker'] != True, None, df['marker_value'])\n",
    "\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "    df = label_page_markers(df)\n",
    "\n",
    "\n",
    "\n",
    "    df= label_page_markers(df)\n",
    "\n",
    "    df = df.drop(columns=['previous_candidate', 'candidate_page_marker', 'marker_type', 'marker_text', 'is_marker', 'marker_type', 'md'])\n",
    "    # rename market value column name to detected_page_number\n",
    "    df = df.rename(columns={'marker_value': 'detected_page_number'})\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [1:05:17<00:00, 39.18s/it]\n"
     ]
    }
   ],
   "source": [
    "folder = 'database/pdfs_train'\n",
    "file_names = os.listdir(folder)\n",
    "\n",
    "# keep only pdf files and extract doc ids\n",
    "file_names = [f[:-4] for f in file_names if f.endswith('.pdf')]\n",
    "\n",
    "file_names = sorted(file_names)\n",
    "\n",
    "# if folder outputs does not exist, create it\n",
    "if not os.path.exists('outputs/train'):\n",
    "    os.makedirs('outputs/train')\n",
    "\n",
    "for file_name in tqdm(file_names):\n",
    "    df = create_df(f'{folder}/' + file_name + '.pdf')\n",
    "    # if a folder with the same name does not exist in oputputs, create it\n",
    "\n",
    "    # if folder with name filename does not exist in outputs, create it\n",
    "    if not os.path.exists(f'outputs/train/{file_name}'):\n",
    "        os.makedirs(f'outputs/train/{file_name}')\n",
    "\n",
    "    df = clean_df(df)\n",
    "    df.to_parquet(f'outputs/train/{file_name}/initial_df.parquet')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ef_thesis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
